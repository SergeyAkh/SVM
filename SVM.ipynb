{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c67f74-e1c1-4f8a-8456-3ee83e96aab3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4faf17-07d0-4e3f-bf9b-d258775dfd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8fb5d87-ac28-4071-b887-ae777cf6613e",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [ 1 - Little bit of theory behind SVM](#1)\n",
    "- [ 2 - <TODO>](#2)\n",
    "- [ 3 - <TODO>](#3)\n",
    "- [ 4 - <TODO>](#4)\n",
    "- [ 5 - <TODO>](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06011b4-5e3b-48fa-b47b-876c1c53788f",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Little bit of theory behind SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdcad72-1ad9-470a-8054-b1c1cbb61b86",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a powerful tool for classification tasks, focusing on finding the best boundary that separates different classes while maximizing the margin\n",
    "The objective of SVM is to find the hyperplane that maximizes the margin between two classes. The equation for the hyperplane can be expressed as:\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "Where:\n",
    "- \\( w \\) is the weight vector.\n",
    "- \\( x \\) is the input feature vector.\n",
    "- \\( b \\) is the bias term.\n",
    "The margin is defined as the distance between the hyperplane and the closest data points from each class, known as support vectors. \n",
    "$$\n",
    "\\frac{1}{2} \\| w \\|^2\n",
    "$$\n",
    "The optimization problem can be formulated as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "        \\min_{w,b,\\xi} \\frac{1}{2} \\| w \\|^2+C\\displaystyle\\sum_{i=1}^l \\xi_i\\\\\n",
    "        y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i\\\\\n",
    "        \\xi_i\\geq0\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "Where:\n",
    "- \\(y_i) is the class label of the training example \\( x_i \\) (either +1 or -1).\n",
    "- \\( i \\) indexes the training examples.\n",
    "- \\(kxi_i) - permission for model to make mistake for the object are inside margin or on the other side of hyperplane\n",
    "- \\(sum_of_kxi_i) - requarement for this mistakes to be as small as possible\n",
    "\n",
    "This is nonlinear optimization problem. For solving this problem we need to use the [`Karush–Kuhn–Tucker (KKT) conditions`](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) and formulate the Lagrangian function in order to solve the dual problem.\n",
    "<br>Let's wright down a Lagrangian of our optimization problem:\n",
    "$$\n",
    "L(w,b,\\xi,\\lambda,\\mu)=\\frac{1}{2} \\| w \\|^2+C\\displaystyle\\sum_{i=1}^l \\xi_i-\\displaystyle\\sum_{i=1}^l \\lambda_i(y_i (w^T x_i + b)-1+\\xi_i)-\\displaystyle\\sum_{i=1}^l \\mu_i\\xi_i\n",
    "$$\n",
    "Were lambda and mu are dual coeffitiants.\n",
    "So let's wright the gradients with respect to each variables.\n",
    "<br>Start from w and look what we'll get:\n",
    "$$\n",
    "\\nabla_w L=w-\\displaystyle\\sum_{i=1}^l \\lambda_iy_ix_i\n",
    ";\\nabla_w L=0=>\n",
    "$$\n",
    "$$\n",
    "w=\\displaystyle\\sum_{i=1}^l \\lambda_iy_ix_i\n",
    "$$ (1)\n",
    "<br>Based on this formula we can see that the solution for the problem is lies in finding lambdas.\n",
    "<br>Now we need to wright gradient (in this case it is partial derivative) with respect to b:\n",
    "$$\n",
    "\\nabla_b L=\\displaystyle\\sum_{i=1}^l \\lambda_iy_i\n",
    ";\\nabla_b L=0\n",
    "$$ (2)\n",
    "<br>And finaly for ksi:\n",
    "$$\n",
    "\\nabla_\\xi L=C-\\lambda_i-\\mu_i\n",
    ";\\nabla_xi L=0=>\n",
    "$$\n",
    "$$\n",
    "\\lambda_i+\\mu_i=C\n",
    "$$ (3)\n",
    "<br>And we also need to wright down co called Complementary slackness that sayes that the constrained functions are equal zero in solution point:\n",
    "<br>This is the first one:\n",
    "$$\n",
    "\\lambda_i(y_i (w^T x_i + b)-1+\\xi_i)=0  =>\n",
    "$$\n",
    "$$\n",
    "\\lambda_i=0 \\lor y_i (w^T x_i + b)=1-\\xi_i\n",
    "$$ (4)\n",
    "<br>And the second one:\n",
    "$$\n",
    "\\mu_i\\xi_i=0=>\n",
    "$$\n",
    "$$\n",
    "\\mu_i=0 \\lor \\xi_i=0\n",
    "$$ (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1670400-638c-432a-abac-6b9e7784df14",
   "metadata": {},
   "source": [
    "Let us now consider the types of objects we have based on our calculated gradients and constraints\n",
    "<br> 1. Lambda_i = 0 => distance from object to hyperplane >= 1 => object was classified right\n",
    "$$\n",
    "\\lambda_i=0; (3) => \\mu_i\\neq 0; (5) => \\xi_i=0; => y_i (w^T x_i + b)\\geq1\n",
    "$$\n",
    "And main conclusion is that x_i, y_i obgect does't influence on the solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbe9e5-3b56-47cc-9ea9-17e40cd3060d",
   "metadata": {},
   "source": [
    "<br>2. 0<lambda_i<C => this object does not cross the margin but lies on that line. This is the Support Vector.\n",
    "$$\n",
    "0<\\lambda_i<C; (3) => \\mu_i=C; (5) => \\xi_i=0; => y_i (w^T x_i + b)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8b4a3-7ebf-4420-a7e7-92d6a96da084",
   "metadata": {},
   "source": [
    "<br>3. Lambda_i = C and xsi_i > 0 => object crosses the magrin line\n",
    "$$\n",
    "\\lambda_i=C; (3) => \\mu_i=0; (5) => \\xi_i>0;\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627b0f7-844e-46a8-afb5-b90cbbfb96ec",
   "metadata": {},
   "source": [
    "Let us put (1) back onto Lagrangian and take into account also (2) and (3). In the end of simplification we'll get this:\n",
    "$$\n",
    "L=\\displaystyle\\sum_{i=1}^l \\lambda_i-\\frac{1}{2}\\displaystyle\\sum_{i,j=1}^l \\lambda_i\\lambda_jy_iy_j<x_i, x_j>\n",
    "$$\n",
    "where\n",
    "$$\n",
    "<x_i, x_j> = x_i^Tx_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e55e63-643d-4a63-9511-11f32cb668b8",
   "metadata": {},
   "source": [
    "Here is a system of equations represented with a system symbol:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "        ax + by = c \\\\\n",
    "        dx + ey = f\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( a, b, c, d, e, f \\) are constants.\n",
    "- \\( x \\) and \\( y \\) are the variables we want to solve for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e39d4-05b2-4914-a7c9-7383ba01fa69",
   "metadata": {},
   "source": [
    "2. The Pythagorean theorem:\n",
    "\n",
    "$$\n",
    "c^2 = a^2 + b^2\n",
    "$$  (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58399923-0683-4651-85c8-45f91093c7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98168f-bb13-46cf-9947-3ef8704e9860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
